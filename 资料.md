## 一．名词解释

深度学习，[稀疏自编码器](https://so.csdn.net/so/search?q=稀疏自编码器&spm=1001.2101.3001.7020)，正则化，集成学习，Dropout

### **1.深度学习：**

深度学习是机器学习的一个分支，其核心是构建具有多个隐藏层的人工神经网络。通过对海量数据进行训练，模型能够自动学习数据从底层到高层的抽象特征表示（Representation Learning），从而解决图像识别、自然语言处理等复杂的非线性问题。

### 2.**稀疏自编码器 (Sparse Autoencoder)** 

这是一种在自编码器损失函数中加入稀疏性限制（通常是KL散度或L1正则化）的无监督学习模型。通过强制隐藏层中大部分神经元处于“非活跃”状态，迫使模型捕捉输入数据中更本质、更鲁棒的特征，而非仅仅复制输入。

### 3.**正则化 (Regularization)** 

正则化是指为了防止模型过拟合（Overfitting）、提高泛化能力而采取的一系列策略。常见方法是在损失函数中增加参数的惩罚项（如L1、L2范数），或在训练过程中随机丢弃神经元（Dropout），以此限制模型的复杂度，使其在未知数据上表现更好。

### 4.**集成学习 (Ensemble Learning)** 

集成学习通过构建并结合多个基学习器（Base Learners）来完成学习任务。其核心思想是利用多个模型的预测结果（通过投票、平均或堆叠等方式），来降低单一模型可能出现的偏差或方差，从而获得比单个模型更优越的泛化性能和稳定性。

### 5.**Dropout** ：

Dropout 是一种在深度神经网络训练中常用的正则化技巧。在每一次训练迭代中，按照一定的概率随机让网络中的一部分神经元暂时“失活”（输出置零且不更新权重）。这能有效打破神经元之间的共适应性（Co-adaptation），显著减少过拟合。

### **6. 卷积神经网络 (Convolutional Neural Network, CNN)**

CNN 是一种专门用于处理具有类似网格结构数据（如图像）的前馈神经网络。它利用卷积层提取局部特征，通过权值共享大幅减少参数量，并结合池化层进行降维，具有良好的平移不变性，是计算机视觉领域的核心模型。

### **7. 循环神经网络 (Recurrent Neural Network, RNN)**

RNN 是一类用于处理序列数据（如文本、时间序列）的神经网络。与传统网络不同，它引入了循环结构（隐藏状态），使得当前时刻的输出不仅取决于当前输入，还取决于之前的计算结果，从而能够捕捉序列中的时间依赖关系。

### **8. 奇异值分解 (Singular Value Decomposition, SVD)**

(注：题目“奇异值外解”应为笔误)

SVD 是一种重要的矩阵分解方法，它将矩阵分解为三个矩阵的乘积（$A = U\Sigma V^T$）。在深度学习中，它常用于模型压缩（对权重矩阵进行低秩近似以减少参数）或数据降维（如 PCA 的底层实现），有时也用于网络的参数初始化。

### **9. 交叉熵 (Cross Entropy)**

交叉熵是深度学习分类任务中常用的损失函数。它用于衡量两个概率分布之间的差异性，即真实标签分布与模型预测分布之间的距离。最小化交叉熵损失，等价于最大化模型对正确类别的预测概率（极大似然估计）。

### **10. 深度信念网络 (Deep Belief Network, DBN)**

DBN 是一种概率生成模型，由多层受限玻尔兹曼机 (RBM) 堆叠而成。其训练过程通常采用“贪婪逐层预训练”策略，即先自底向上逐层训练每一层 RBM，再通过反向传播对整个网络进行微调，常用于无监督特征学习或初始化深层网络。

### **11. 相对熵 (Relative Entropy)**

又称为 KL散度 (Kullback-Leibler Divergence)，它是衡量两个概率分布（例如真实分布 $P$ 和模型预测分布 $Q$）之间差异的一种非对称性指标。在深度学习中，相对熵的值越小，说明两个分布越接近；最小化相对熵往往等价于最小化交叉熵损失。

### **12. 欠拟合 (Underfitting)**

指模型的复杂度过低或学习能力不足，导致无法捕捉数据中的基本规律和特征。其典型表现是模型在训练集和测试集上的误差都非常大（高偏差）。通常的解决办法包括增加网络层数、增加神经元数量或减少正则化约束。

### **13. 深度森林 (Deep Forest)**

这是一种基于决策树集成的非神经网络深度模型（如 gcForest）。它借鉴了深度神经网络的级联层级结构，但每一层由多个随机森林组成。通过逐层特征变换和表示学习，它具备深度模型的表征能力，同时拥有参数少、易于训练且不需要大量反向传播的特点。

### **14. 降噪自编码器 (Denoising Autoencoder, DAE)**

这是自编码器的一种变体，旨在学习更鲁棒的特征。在训练时，它人为地向输入数据加入噪声（如随机遮挡或高斯噪声），然后强迫网络从损坏的输入中还原出原始的纯净数据。这种机制迫使模型捕捉数据本质的依赖关系，而非简单地复制输入。

### **15. 胶囊网络 **

胶囊网络旨在解决传统 CNN 无法捕捉物体空间层级关系（如五官相对位置）及视角变化的问题。它使用输出为向量的“胶囊”代替标量神经元，向量的模长代表特征存在的概率，方向代表姿态参数。通过动态路由 (Dynamic Routing) 算法替代池化层，能够更好地保留物体的整体与局部关系。

### **16. 深度可分离卷积**

这是一种旨在减少计算量和参数量的卷积结构，核心应用于 MobileNet 等轻量化模型。它将标准卷积分解为两个步骤：深度卷积 (Depthwise)（对每个输入通道独立进行卷积）和 逐点卷积 (Pointwise)（使用 1x1 卷积核融合通道信息）。这种分解显著提升了模型的运行效率。

### **17. 目标检测 (Object Detection)**

目标检测是计算机视觉的核心任务之一，其目标是解决图像中“有什么”和“在哪里”的问题。模型不仅需要对图像中的多个物体进行分类（Classification），还需要回归出物体的边界框（Bounding Box）进行定位（Localization）。常见算法分为 Two-Stage（如 Faster R-CNN）和 One-Stage（如 YOLO）两类。

### **18 焦点损失 (Focal Loss)**

Focal Loss 是为了解决目标检测（特别是 One-Stage 检测器）中正负样本极度不平衡以及简单样本主导梯度的问题而提出的损失函数。它在标准交叉熵损失的基础上增加了一个调节因子 $(1-p_t)^\gamma$，降低了易分类样本（简单负样本）的权重，迫使模型专注于训练那些难以分类的“硬样本”。

### **19. 注意力机制 (Attention Mechanism)** 

注意力机制模仿人类视觉的选择性关注能力，旨在让神经网络在处理大量信息时，能够动态地赋予输入数据的不同部分以不同的权重。它通过计算查询（Query）与键（Key）的相关性来加权值（Value），使模型专注于对当前任务最关键的信息，是 Transformer 模型的核心组件。

### **20. 自编码器 (Autoencoder)** 

这是一种无监督学习的神经网络模型，由**编码器 (Encoder)** 和 **解码器 (Decoder)** 两部分组成。其目标是学习数据的低维压缩表示（隐变量），使得输出能够尽可能逼真地重构输入数据。常用于数据降维、特征提取以及图像去噪。

### **21.感受野 (Receptive Field)**

 在卷积神经网络 (CNN) 中，感受野是指特征图（Feature Map）上某个特定神经元在原始输入图像上所能“看到”或映射的区域大小。感受野越大，神经元能接触到的上下文信息越广；感受野越小，则更关注局部细节特征。它随网络层数的加深而逐层扩大。

### **22.残差连接 (Residual Connection)** 

残差连接是 ResNet 的核心结构，通过引入“跳跃连接”（Skip Connection），将输入 $x$ 直接加到堆叠层的输出上，即学习残差映射 $H(x) = F(x) + x$。这种结构创造了梯度反向传播的“高速公路”，有效解决了深层网络中的梯度消失问题，使得训练极深的网络成为可能。

### **23. LSTM (长短期记忆网络)** 

LSTM 是循环神经网络 (RNN) 的一种特殊变体，专门设计用于解决长序列训练中的梯度消失和长时依赖问题。它引入了**细胞状态**作为信息传递的主干，并通过**遗忘门、输入门、输出门**三种门控结构来精确控制信息的保留、添加和输出，从而能够有效捕捉时间序列中距离较长的依赖关系。

### **24. 过拟合 (Overfitting)** 

过拟合是指机器学习模型在训练集上表现极好（误差很小），但在测试集或未知数据上表现较差（泛化能力弱）的现象。这通常是因为模型复杂度过高，导致它不仅学习了数据背后的规律，还“死记硬背”了训练数据中的随机噪声。常见的解决方法包括增加训练数据、使用正则化（Regularization）、Dropout 或早停法 (Early Stopping)。

## 二．简答题（每题5分，共30分）

### 1．请简述你对误差反向传播算法的理解。

BP算法是训练神经网络的核心，其本质是**链式法则（Chain Rule）**在计算图中的应用。

它分为两个过程：

- **前向传播**：输入数据经过网络层层计算，得到预测值，并计算与真实标签的损失（Loss）。
- **反向传播**：将损失函数 $L$ 对权重的梯度 $\frac{\partial L}{\partial w}$ 从输出层向输入层逐层反向传递。

核心公式依据链式法则：
$$
\frac{\partial L}{\partial w} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial x} \cdot \frac{\partial x}{\partial w}
$$


最终利用梯度下降算法（如$w \leftarrow w - \eta \frac{\partial L}{\partial w}$）更新权重，以最小化损失。
### 2．请列出卷积神经网络的主要结构模块，以及各个模块完成的功能。

**卷积层 (Convolutional Layer)**：核心模块。利用卷积核（Filter）进行滑动窗口计算。**功能**：提取图像的局部特征（如边缘、纹理），通过“权值共享”减少参数量。

**激活层 (Activation Layer)**：通常使用 ReLU。**功能**：引入非线性因素，使网络具备拟合复杂非线性函数的能力。

**池化层 (Pooling Layer)**：通常是 Max Pooling 或 Average Pooling。**功能**：下采样（Downsampling），减少特征图尺寸和计算量，同时保持特征的平移不变性。

**全连接层 (Fully Connected Layer)**：通常位于尾部。**功能**：将提取的高维特征展平，综合所有特征信息进行最终的分类或回归输出。

### 3．请简述你对LSTM的理解，并解释为什么它能够解决长时依赖问题。

**理解**：LSTM（长短期记忆网络）是 RNN 的一种变体。它引入了**细胞状态（Cell State, $C_t$）**作为信息传递的主干道，并通过三个“门”结构（**遗忘门** $f_t$、**输入门** $i_t$、**输出门** $o_t$）来精确控制信息的保留、添加和输出。

解决长时依赖的原因：

标准 RNN 在反向传播时涉及连乘，容易导致梯度消失或爆炸。

LSTM 的细胞状态更新公式主要为加法运算：$$C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t$$

这使得梯度在反向传播时能够长时间维持而不容易变为 0（即“恒误差这一概念”），从而能捕捉到序列中距离较远的信息关联。

### 4．请简述深度学习中常见的避免过拟合的方法。

过拟合是指模型在训练集上表现极好，但在测试集上表现差。常见的解决方法包括：

- **数据增强 (Data Augmentation)**：通过旋转、裁剪、翻转等方式扩充训练数据，增加数据多样性。
- **正则化 (Regularization)**：在损失函数中加入 $L1$ 或 $L2$ 范数惩罚项，限制权重数值过大，降低模型复杂度。
- **Dropout**：在训练过程中按一定概率随机让神经元“失活”，减少神经元间的依赖，防止对局部特征的过分拟合。
- **早停法 (Early Stopping)**：监控验证集的误差，当验证集误差不再下降反而上升时，提前停止训练。

### 5．请简述你对生成对抗网络的理解，并简述其训练过程。

**理解**：GAN 2 包含两个相互博弈的神经网络：**生成器 ($G$)** 和 **判别器 ($D$)**。

- 生成器试图从随机噪声中生成逼真的样本来“欺骗”判别器。
- 判别器试图区分输入是真实样本还是生成器生成的假样本。
- 两者构成一个“零和博弈”，最终目标是达到纳什均衡，使生成的数据无法被区分（$D$ 的输出概率接近 0.5）。

**训练过程**（交替迭代）：

1. **固定 $G$，训练 $D$**：将真实数据标记为 1，生成数据标记为 0，最大化 $D$ 的分类准确率。
2. **固定 $D$，训练 $G$**：输入随机噪声生成样本，标记为 1（意图欺骗 $D$），最小化 $D$ 将其判为假的概率（或最大化判为真的概率）。
3. 重复上述步骤，直到生成的样本足够逼真。

### 6．请简述你对胶囊网络的理解。

**核心思想**：为了解决传统 CNN 在处理物体空间层级关系（如五官的位置关系）和视角变化时的不足（即“皮卡索问题”）而提出。

**向量神经元**：传统神经元输出标量，而胶囊网络的“胶囊”输出**向量**。向量的**模长**代表特征存在的概率，**方向**代表特征的姿态参数（如位置、旋转、大小）。

**动态路由 (Dynamic Routing)**：替代了 CNN 的池化层。低层胶囊通过“协议路由”算法（Routing by Agreement），将输出发送给那些预测结果与自身一致的高层胶囊，从而更好地保留物体的整体与局部关系。



### 7. 请简述反向传播算法的思想，并用图和公式说明其过程。

**核心思想**： 反向传播（Backpropagation）是训练神经网络的核心。其本质是利用**链式法则（Chain Rule）**，将输出层的误差（Loss）对权重的梯度从后向前逐层传递，从而计算出每一层参数的梯度，最后配合梯度下降算法更新权重，以最小化损失函数。

<img src="Z:\课程作业\深度学习\资料\img\image-20251130203759793.png" alt="image-20251130203759793" style="zoom: 50%;" />

**求 $w_2$ 的梯度：**$\frac{\partial L}{\partial w_2} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial w_2}$

**求 $w_1$ 的梯度**（误差回传至隐藏层）：$\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial h} \cdot \frac{\partial h}{\partial w_1}$

**权重更新**：$w \leftarrow w - \eta \frac{\partial L}{\partial w}$ （其中 $\eta$ 为学习率）。



### 8. 什么是过拟合和欠拟合？如何解决这两种问题？

**定义**：

- **欠拟合 (Underfitting)**：模型复杂度太低，无法捕捉数据的特征，导致在训练集和测试集上误差都很大。
- **过拟合 (Overfitting)**：模型复杂度过高，死记硬背了训练数据中的噪声，导致训练集误差极低，但测试集误差很高（泛化能力差）。

**解决方法**：

- **解决欠拟合**：
  1. 增加模型复杂度（如加深网络层数、增加神经元数量）。
  2. 减少正则化参数的惩罚力度。
  3. 增加特征输入或进行特征工程。
- **解决过拟合**：
  1. **数据增强 (Data Augmentation)**：增加训练样本的多样性。
  2. **正则化 (Regularization)**：加入 L1/L2 范数惩罚项。
  3. **Dropout**：随机丢弃神经元。
  4. **早停法 (Early Stopping)**：验证集误差上升时停止训练。

### 9. 请简述 Yolo 算法的主要思想和实现过程。

**主要思想：**YOLO (You Only Look Once) 将目标检测视为一个回归问题。它不像 R-CNN 那样需要生成候选区域 (Region Proposal)，而是直接在整个图像上预测边界框和类别，速度极快（One-Stage）。

**实现过程**：

1. **网格划分**：将输入图像划分为 $S \times S$ 的网格（Grid Cell）。
2. **预测**：如果一个物体的中心落入某个网格，该网格负责检测该物体。每个网格预测 $B$ 个边界框（Bounding Box），包含位置 $(x, y, w, h)$ 和置信度 (Confidence)，以及 $C$ 个类别的概率。
3. **输出张量**：最终输出维度为 $S \times S \times (5 \times B + C)$。
4. **后处理**：通过非极大值抑制（NMS）去除重叠度高且置信度低的框，得到最终检测结果。

### 10. 请简述GRU网络的主要思想，并用图和公式表达其计算过程.

GRU (Gated Recurrent Unit) 是 LSTM 的一种变体，旨在解决 RNN 的梯度消失问题。它将 LSTM 的三个门简化为两个门：**更新门 (Update Gate, $z_t$)** 和 **重置门 (Reset Gate, $r_t$)**，并将细胞状态与隐藏状态合并。它通过门控机制控制信息流，能够捕捉长序列依赖，同时参数量比 LSTM 更少，训练效率更高。

<img src="Z:\课程作业\深度学习\资料\img\image-20251130204706282.png" alt="image-20251130204706282" style="zoom:33%;" />



<img src="Z:\课程作业\深度学习\资料\img\image-20251130204737927.png" alt="image-20251130204737927" style="zoom:33%;" />



### 11. 请简述胶囊网络的主要思想，并用图和公式表达其计算过程。

**主要思想**： 为了克服 CNN 无法处理物体空间层级关系（如视角变化、五官相对位置）的缺陷（即标量神经元丢失了空间信息），Hinton 提出了胶囊网络。

- **向量神经元**：使用“胶囊”（向量）代替标量，向量长度代表特征存在的概率，方向代表姿态。
- **动态路由 (Dynamic Routing)**：替代池化层，低层胶囊通过“协议路由”算法将输出发送给预测一致的高层胶囊。

<img src="Z:\课程作业\深度学习\资料\img\image-20251130205323174.png" alt="image-20251130205323174" style="zoom: 50%;" />

-  (Input Capsule):

  输入层的低层胶囊向量，代表低层特征（如眼睛、鼻子）。

- $W$ (Affine Transform):

  权重矩阵 $W_{ij}$。它将低层特征 $u_i$ 转换为对高层特征的预测向量 $\hat{u}_{j|i}$ (即 $\hat{u}_{j|i} = W_{ij} u_i$)。

- $c$ (Coupling Coefficient):

  路由系数（耦合系数）。由动态路由算法计算得出，表示低层胶囊 $i$ 认为自己属于高层胶囊 $j$ 的概率权重。

- $\Sigma$ (Sum / Input Vector $s_j$):

  加权求和过程。高层胶囊的输入 $s_j$ 是所有低层预测向量的加权和 ($s_j = \sum_i c_{ij} \hat{u}_{j|i}$)。

- Squash (Activation Function):

  非线性激活函数。将向量 $s_j$ 的模长压缩到 0 到 1 之间，同时保持方向不变，输出最终的高层胶囊 $v_j$。

  - *功能*：模长代表“实体存在的概率”，方向代表“实体的姿态”。

- $v_j$ (Output Capsule):

  输出层的高层胶囊向量（如面部）。

### 12. 请简述生成对抗网络的主要原理，并用公式表达其目标函数.

主要原理：

生成对抗网络 (GAN) 基于博弈论思想，由两个相互对抗的神经网络组成：

1. **生成器 (Generator, $G$)**：输入随机噪声 $z$，试图生成足够逼真的样本 $G(z)$ 来“欺骗”判别器。

2. 判别器 (Discriminator, $D$)：接收真实数据 $x$ 和生成数据 $G(z)$，试图准确区分哪一个是真的，哪一个是假的（二分类问题）。

   两者构成一个零和博弈 (Zero-Sum Game)：$D$ 越强，$G$ 就必须越强才能骗过它。最终目标是达到纳什均衡，使生成的数据分布拟合真实数据分布，让判别器无法区分（输出概率接近 0.5）。

<img src="Z:\课程作业\深度学习\资料\img\image-20251130205921539.png" alt="image-20251130205921539" style="zoom: 50%;" />

<img src="Z:\课程作业\深度学习\资料\img\image-20251130205943556.png" alt="image-20251130205943556" style="zoom: 50%;" />



### 13．请简述Dropout的实现方式，并阐述你理解的它对于解决过拟合问题的原因。

**实现方式：**在训练过程中，按照设定的概率 $p$（如 0.5）随机将网络层中一部分神经元的输出置为 0（即暂时“丢弃”）。为了保证训练和测试时输出期望一致，通常采用 Inverted Dropout：在训练时将保留下来的神经元数值除以 $1-p$ 进行放大，测试时则保持原样不做处理。

**解决过拟合的原因**：

1. **集成学习效应 (Ensemble)**：每次迭代都在训练不同的“子网络”，最终的模型相当于海量不同结构子网络的平均，降低了模型的方差。
2. **打破特征共适应 (Co-adaptation)**：神经元不能依赖于特定的其他神经元存在，被迫学习更加鲁棒、独立的特征表示。

### 14．请简述你对Batch Normalization的理解，并说明其在训练和测试阶段如何实现？

**理解：**BN 旨在解决深层网络训练中的 内部协变量偏移 (Internal Covariate Shift) 问题。它通过将每一层的输入归一化为均值 0、方差 1 的分布，并引入可学习参数 $\gamma$（缩放）和 $\beta$（平移）来恢复表达能力。它能加速收敛，允许更大的学习率，并具有一定的正则化效果。

**训练与测试的区别**：

- **训练阶段**：利用**当前 Mini-Batch** 的数据计算均值 $\mu_B$ 和方差 $\sigma^2_B$ 进行归一化。同时，使用指数加权平均（Moving Average）动态维护全局的均值 $\mu_{running}$ 和方差 $\sigma^2_{running}$。
- **测试阶段**：不使用当前测试样本的统计量（样本太少不稳定），而是直接使用训练期间积累的**全局统计量** ($\mu_{running}, \sigma^2_{running}$) 进行归一化。

### 15．请简述你对生成对抗网络的理解，并简述其训练过程。

**理解：**GAN 基于博弈论思想，包含两个相互对抗的网络：生成器 ($G$) 试图从噪声中生成逼真样本欺骗判别器；判别器 ($D$) 试图区分真实样本和生成样本。两者的目标是达到纳什均衡，即生成的分布拟合真实分布。

**训练过程**（交替迭代）：

1. **固定 $G$，训练 $D$**：将真实样本标为 1，生成样本标为 0，最大化 $D$ 的分类准确率。
2. **固定 $D$，训练 $G$**：输入噪声生成样本，并将其标签视为 1（意图欺骗），最小化 $D$ 将其判为假的概率（或最大化 $D$ 判为真的概率）。

### 16．请简述你对残差网络的理解，并解释为什么它能够解决梯度消失问题。

**理解：**ResNet 引入了 跳跃连接 (Shortcut Connection)，让网络学习残差映射 $H(x) = F(x) + x$，而不是直接学习潜在映射。这使得网络更容易学习恒等映射（Identity Mapping），从而能够训练极深的网络（如 152 层）。

**解决梯度消失的原因：**根据链式法则，梯度在反向传播时通过加法结构直接回流：

$$\frac{\partial Loss}{\partial x} = \frac{\partial Loss}{\partial H} \cdot \frac{\partial H}{\partial x} = \frac{\partial Loss}{\partial H} \cdot (\frac{\partial F}{\partial x} + 1)$$

公式中的 "$+1$" 像一条“高速公路”，保证了即使深层卷积部分的梯度 $\frac{\partial F}{\partial x}$ 趋近于 0，总梯度也能保持在 1 附近传回浅层，有效避免了连乘导致的梯度消失。

### 17. 请写出对矩阵 $A_{m \times n}$ ($m \neq n$) 进行奇异值分解的过程。

奇异值分解（SVD）是将矩阵 $A$ 分解为 $A = U \Sigma V^T$ 的形式。

分解过程：

1. **计算 $A A^T$ 和 $A^T A$**：得到两个对称方阵。
2. **求左奇异向量 $U$**：计算 $A A^T$ 的特征向量，构成正交矩阵 $U$ ($m \times m$)。
3. **求右奇异向量 $V$**：计算 $A^T A$ 的特征向量，构成正交矩阵 $V$ ($n \times n$)。
4. **求奇异值矩阵 $\Sigma$**：求 $A^T A$ 的特征值 $\lambda_i$，计算奇异值 $\sigma_i = \sqrt{\lambda_i}$。将 $\sigma_i$ 按从大到小排列填入对角线，其余位置补 0，构造矩阵 $\Sigma$ ($m \times n$)。
5. **组合**：得到 $A = U \Sigma V^T$。

### 18. 请图示说明卷积神经网络的主要组成部分及其功能。

图示说明：

(考场手绘建议：画一个简单的流程图)

$$\text{Input Image} \xrightarrow{\text{卷积}} \text{[Feature Map]} \xrightarrow{\text{激活}} \xrightarrow{\text{池化}} \text{[Downsampled Map]} \dots \xrightarrow{\text{全连接}} \text{Output}$$

**主要组成与功能：**

1. **卷积层 (Convolutional Layer)**：利用卷积核进行滑动窗口计算。**功能**：提取图像的局部特征（如边缘、纹理），并通过权值共享减少参数。
2. **激活层 (Activation Layer, e.g., ReLU)**：$f(x) = \max(0, x)$。**功能**：引入非线性，使网络具备拟合复杂函数的能力。
3. **池化层 (Pooling Layer)**：如最大池化。**功能**：下采样，降低特征维度，减少计算量，并提供平移不变性。
4. **全连接层 (Fully Connected Layer)**：**功能**：汇总提取的特征，映射到样本标记空间进行分类或回归。

### 19. 请简述神经网络模型中 Dropout 正则化方法的主要思想并图示说明。

主要思想：在训练过程中，按照一定的概率 $p$ 随机“临时丢弃”（输出置零）神经网络中的部分神经元。

这能够防止神经元之间的共适应 (Co-adaptation)，迫使网络学习更鲁棒的特征。同时，Dropout 相当于在训练无数个不同的子网络，测试时使用完整网络（权重缩放），起到了模型集成 (Ensemble) 的效果，从而有效抑制过拟合。

<img src="Z:\课程作业\深度学习\资料\img\image-20251130222451824.png" alt="image-20251130222451824" style="zoom:33%;" />

### 20. 请简述随机梯度下降法 (SGD) 的基本思想并图示说明。

基本思想：标准梯度下降 (BGD) 每次更新参数都要计算所有样本的梯度，速度慢。

随机梯度下降 (SGD) 每次迭代仅使用一个样本（或一个小批量 Mini-batch）来计算梯度并更新参数：

$$\theta = \theta - \eta \cdot \nabla_\theta J(\theta; x^{(i)}, y^{(i)})$$

其优点是计算快，能在线学习，且随机性有助于跳出局部最优解；缺点是更新路径存在震荡（噪音）。

图示说明：

<img src="Z:\课程作业\深度学习\资料\img\image-20251130222700378.png" alt="image-20251130222700378" style="zoom: 33%;" />

- **BGD**：是一条垂直于等高线、平滑指向圆心的直线。
- **SGD**：是一条**折线 (Zigzag)**，虽然方向不断波动，但总体趋势向圆心逼近。

### 21. 请简述 Transformer 的主要思想，并用图和公式说明。

主要思想：Transformer 摒弃了传统的循环 (RNN) 和卷积 (CNN) 结构，完全基于 自注意力机制 (Self-Attention)。它通过多头注意力 (Multi-Head Attention) 并行捕捉序列中任意两个位置之间的全局依赖关系，解决了 RNN 难以并行计算和长距离依赖的问题。

核心公式 (Scaled Dot-Product Attention)：$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$

其中 $Q$ (Query), $K$ (Key), $V$ (Value) 均由输入向量线性变换而来。

图示说明：

(考场手绘建议：画出单个 Encoder Layer 的结构)

<img src="Z:\课程作业\深度学习\资料\img\image-20251130223006789.png" alt="image-20251130223006789" style="zoom:33%;" />

### 22. 请简述 R-CNN (R-CNN, Fast R-CNN, Faster R-CNN) 系列工作的改进思路。

**答：** 该系列的核心改进思路是**不断减少重复计算**并实现**端到端训练**：

- **R-CNN**：采用“Select Search + CNN + SVM”的分阶段流程。缺点是每个候选框独立经过 CNN，计算冗余度极高，训练繁琐。
- **Fast R-CNN**：引入 **RoI Pooling (感兴趣区域池化)**。让所有候选框**共享**同一张整图的特征图，只需对整图做一次卷积，大幅提升速度；同时使用 Softmax 替代 SVM 实现分类与回归的多任务损失统一训练。
- **Faster R-CNN**：引入 **RPN (区域生成网络)**。用全卷积神经网络替代了慢速的 Select Search 算法来生成候选框，实现了候选框生成与检测网络的特征共享，真正达成了**端到端 (End-to-End)** 的实时检测。

### 23. 请简述你对 Transformer 网络的理解，并解释为处理图像信号 Swin Transformer 做了哪些改进，分别解决什么问题。

**答：** **Transformer 理解**： Transformer 是一种基于 **自注意力机制 (Self-Attention)** 的架构。它摒弃了循环和卷积，通过并行计算捕捉序列中任意两个位置之间的**全局依赖关系**，特别擅长处理长距离上下文信息。

**Swin Transformer 的改进与解决的问题**： 针对图像分辨率高（计算量大）且物体尺度变化大的特点，Swin 做了以下改进：

1. **基于窗口的注意力 (Window MSA)**：
   - **改进**：将自注意力计算限制在固定的局部窗口（Windows）内，而非全图。
   - **解决问题**：解决了标准 Transformer 在高分辨率图像上计算复杂度呈平方级爆炸的问题（降低为线性复杂度）。
2. **移动窗口机制 (Shifted Window)**：
   - **改进**：在连续层之间移动窗口的划分边界。
   - **解决问题**：解决了独立窗口之间缺乏信息交互的问题，实现了跨窗口的特征传递，从而恢复全局感受野。
3. **层级式结构 (Patch Merging)**：
   - **改进**：像 CNN 一样逐层合并 Patch（下采样）。
   - **解决问题**：构建了多尺度特征金字塔，使其能像 CNN 一样有效检测不同尺度的物体。

### 24.简述语言模型的发展历程。

**统计语言模型 (SLM)**：以 N-gram 为代表，主要基于马尔可夫假设，通过统计文本中词序列出现的概率来预测下一个词。虽然理论基础扎实，但受限于“维数灾难”，存在严重的数据稀疏问题，且难以捕捉长距离的语义依赖。

**神经语言模型 (NLM)**：以 Word2Vec 和 RNN/LSTM 为代表，核心突破是引入了神经网络和“分布式表示（词向量）”。它将离散的词映射为低维稠密的连续向量，有效解决了数据稀疏问题，并能捕捉词与词之间的语义相似性。

**预训练语言模型 (PLM)**：以 BERT 和 GPT-1/2 为代表，基于 Transformer 架构和自注意力机制。它确立了“大规模无监督预训练 + 下游任务微调 (Fine-tuning)”的范式，能够充分利用海量无标注数据学习深层的双向上下文信息，显著提升了各类 NLP 任务的性能。

**大语言模型 (LLM)**：以 GPT-3、ChatGPT 为代表，其特征是参数量和数据量达到了极大的规模。模型表现出“涌现 (Emergent)”能力，不再依赖特定的微调，而是转向“提示工程 (Prompt Learning)”，具备了强大的零样本学习、复杂逻辑推理和通用生成能力。



## 三．计算题（每题10分，共20分）

### 1．请使用卷积神经网络中的Full卷积、Same卷积和Valid卷积分别计算下图所示输入矩阵和卷积核对应的特征图，卷积步长为1，激活函数采用ReLU。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/7b449f6db3ed2509116a357cfc23a9a7.png)

<img src="Z:\课程作业\深度学习\img\image-20251130192816637.png" alt="image-20251130192816637" style="zoom:50%;" />![image-20251130193314260](Z:\课程作业\深度学习\img\image-20251130193314260.png)<img src="Z:\课程作业\深度学习\img\image-20251130192816637.png" alt="image-20251130192816637" style="zoom:50%;" />![image-20251130193314260](Z:\课程作业\深度学习\img\image-20251130193314260.png)

### 2．多分类任务中，某个样本的期望输出为（0，0，0，1），两个模型A和B都采用交叉熵作为损失函数，针对该样本的实际输出分别为（In20，In40，In60，In80）、（In10，In30，ln50，In90），采用Softmax 函数对输出进行归一化并计算两个模型的交叉熵，说明哪个模型更好。提示：lg2≈0.301，lg3≈0.477。

<img src="Z:\课程作业\深度学习\资料\img\image-20251130200651246.png" alt="image-20251130200651246" style="zoom: 33%;" />

<img src="Z:\课程作业\深度学习\资料\img\image-20251130200801994.png" alt="image-20251130200801994" style="zoom:33%;" />



### 3， 请使用卷积神经网络中的Full卷积、Same卷积和Valid卷积分别计算下图所示输入矩阵和卷积核对应的特征图，卷积步长为1，激活函数采用ReLU.

### 4. 二分类任务中，样本（5个）的期望输出（类标签）如下图左侧矩阵所示，对应的实际输出下图右侧矩阵所示，模型采用交叉熵作为损失函数，计算：

**（1） 模型的交叉熵损失；**

**（2） 模型的焦点损失（Focal loss），其中y= 2， a = 0.4.**

###### 

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/bb7a9b27d94b765a4b925fcf367541fb.png)

<img src="Z:\课程作业\深度学习\资料\img\image-20251130210147492.png" alt="image-20251130210147492" style="zoom: 33%;" />

对于单个样本 $i$，二分类交叉熵损失为：

$$L_i = -[y_i \ln(p_i) + (1-y_i) \ln(1-p_i)]$$

或者更简单地理解：直接对正确类别的预测概率取对数并取反。

即：如果 $y=1$，损失是 $-\ln(p)$；如果 $y=0$，损失是 $-\ln(1-p)$。

#### （1）计算模型的交叉熵损失 (Binary Cross Entropy Loss)

公式：对于单个样本 $i$，二分类交叉熵损失为：

$$L_i = -[y_i \ln(p_i) + (1-y_i) \ln(1-p_i)]$$

或者更简单地理解：直接对正确类别的预测概率取对数并取反。

即：如果 $y=1$，损失是 $-\ln(p)$；如果 $y=0$，损失是 $-\ln(1-p)$。

**逐个样本计算**：

1. **样本 1** ($y=0, p=0.2$)：
   - 正确类别的概率为 $1 - 0.2 = 0.8$
   - $L_1 = -\ln(0.8) \approx 0.2231$
2. **样本 2** ($y=1, p=0.8$)：
   - 正确类别的概率为 $0.8$
   - $L_2 = -\ln(0.8) \approx 0.2231$
3. **样本 3** ($y=0, p=0.4$)：
   - 正确类别的概率为 $1 - 0.4 = 0.6$
   - $L_3 = -\ln(0.6) \approx 0.5108$
4. **样本 4** ($y=0, p=0.1$)：
   - 正确类别的概率为 $1 - 0.1 = 0.9$
   - $L_4 = -\ln(0.9) \approx 0.1054$
5. **样本 5** ($y=1, p=0.9$)：
   - 正确类别的概率为 $0.9$
   - $L_5 = -\ln(0.9) \approx 0.1054$

计算总平均损失：

$$L_{mean} = \frac{1}{5} (L_1 + L_2 + L_3 + L_4 + L_5)$$

$$L_{mean} = \frac{1}{5} (0.2231 + 0.2231 + 0.5108 + 0.1054 + 0.1054)$$

$$L_{mean} = \frac{1}{5} (1.1678) \approx \mathbf{0.2336}$$

#### （2）计算模型的焦点损失 (Focal Loss)

公式：$FL(p_t) = -\alpha_t (1 - p_t)^\gamma \ln(p_t)$

**参数定义**：

- $\gamma = 2$
- $\alpha = 0.4$
- $p_t$：模型对**真实类别**的预测概率。
- $\alpha_t$：平衡系数。
  - 当 $y=1$ 时，$\alpha_t = \alpha = 0.4$
  - 当 $y=0$ 时，$\alpha_t = 1 - \alpha = 0.6$



### 5．如下图卷积神经网络所示：

卷积层C1为3x3大小的卷积核，卷积层深度为5，Stride＝1，卷积层C2为5x5大小的卷积核，卷积层深度为2，stride＝1，卷积方式均为Valid卷积；池化层P1为2x2大小的均值池化，stride＝2；输出层是10x1的向量；请计算输出特征图F1、F2和F3的大小（宽x高x通道数），特征图F3的感受野大小，并分别计算卷积层和全连接层的参数量（其中卷积和全连接操作均不考虑偏置参数，提示：注意卷积层深度的概念）。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/5b1faa834e870ef656908158dbbcdb32.png)



#### 一、 计算输出特征图的大小

计算公式为：

$$Output = \frac{Input - Kernel + 2 \times Padding}{Stride} + 1$$

(注：深度 Depth 即为通道数 Channels)

**1. 计算 F1 (C1层的输出)**

- **输入**：$28 \times 28 \times 1$

- **参数**：卷积核 $3 \times 3$，步长 $S=1$，Valid卷积 ($P=0$)，深度(卷积核个数) $= 5$

- 计算：

  $$Width/Height = \frac{28 - 3 + 0}{1} + 1 = 26$$

  $$Channels = 5$$

- **结果 F1**：$\mathbf{26 \times 26 \times 5}$

**2. 计算 F2 (P1层的输出)**

- **输入**：$26 \times 26 \times 5$ (即 F1)

- **参数**：池化核 $2 \times 2$，步长 $S=2$

- 计算：

  $$Width/Height = \frac{26 - 2}{2} + 1 = 12 + 1 = 13$$

  $$Channels = 5 \text{ (池化不改变通道数)}$$

- **结果 F2**：$\mathbf{13 \times 13 \times 5}$

**3. 计算 F3 (C2层的输出)**

- **输入**：$13 \times 13 \times 5$ (即 F2)

- **参数**：卷积核 $5 \times 5$，步长 $S=1$，Valid卷积 ($P=0$)，深度(卷积核个数) $= 2$

- 计算：

  $$Width/Height = \frac{13 - 5 + 0}{1} + 1 = 9$$

  $$Channels = 2$$

- **结果 F3**：$\mathbf{9 \times 9 \times 2}$

#### 二、 计算特征图 F3 的感受野大小

感受野计算通常采用从后向前推导的方法。

公式：$RF_{in} = (RF_{out} - 1) \times Stride + Kernel$

1. **从 F3 (C2输出) 看 F2 (C2输入)**：
   - F3 上的 1 个点对应 F2 上的区域。
   - $RF_{F2} = (1 - 1) \times 1 + 5 = 5$
   - 即 F3 上的 1 个点看 F2 上的 $5 \times 5$ 区域。
2. **从 F2 (P1输出) 看 F1 (P1输入)**：
   - $RF_{F1} = (RF_{F2} - 1) \times Stride_{pool} + Kernel_{pool}$
   - $RF_{F1} = (5 - 1) \times 2 + 2 = 4 \times 2 + 2 = 10$
   - 即 F3 上的 1 个点看 F1 上的 $10 \times 10$ 区域。
3. **从 F1 (C1输出) 看 原始图像 (C1输入)**：
   - $RF_{input} = (RF_{F1} - 1) \times Stride_{conv1} + Kernel_{conv1}$
   - $RF_{input} = (10 - 1) \times 1 + 3 = 9 + 3 = 12$

**结果**：特征图 F3 的感受野大小为 $\mathbf{12 \times 12}$。

#### 三、 计算参数量 (不考虑偏置)

参数量公式：

- 卷积层：$Kernel \times Kernel \times C_{in} \times C_{out}$
- 全连接层：$N_{in} \times N_{out}$

**1. 卷积层 C1 参数量**

- 输入通道 $C_{in} = 1$ (灰度图)
- 卷积核 $3 \times 3$，输出通道(深度) $C_{out} = 5$
- 计算：$3 \times 3 \times 1 \times 5 = \mathbf{45}$

**2. 卷积层 C2 参数量**

- 输入通道 $C_{in} = 5$ (来自 F2)
- 卷积核 $5 \times 5$，输出通道(深度) $C_{out} = 2$
- 计算：$5 \times 5 \times 5 \times 2 = 25 \times 10 = \mathbf{250}$

**3. 全连接层 S2 参数量**

- 输入节点数 $N_{in}$：将 F3 展平 (Flatten)。

  $$N_{in} = Width \times Height \times Channels = 9 \times 9 \times 2 = 162$$

- 输出节点数 $N_{out} = 10$

- 计算：$162 \times 10 = \mathbf{1620}$



### 6．根据表格中的数据使用ID3算法构建决策树，预测西瓜好坏，给出每步的计算过程（信息熵计算以2为底数）。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/661735e498e5731334a1ddb64e0b0705.png)

<img src="Z:\课程作业\深度学习\资料\img\image-20251130215636183.png" alt="image-20251130215636183" style="zoom:33%;" />

#### 第一步：计算根节点的信息熵

首先统计数据集 $D$ 中的样本情况：

- **总样本数** $|D| = 11$
- **好瓜（正例）**：ID {1, 2, 5, 6, 7}，共 5 个，占比 $p_1 = 5/11$
- **坏瓜（负例）**：ID {3, 4, 8, 9, 10, 11}，共 6 个，占比 $p_2 = 6/11$

根节点的信息熵 $Ent(D)$ 计算如下：

$$Ent(D) = - \sum_{k=1}^{2} p_k \log_2 p_k = - (\frac{5}{11} \log_2 \frac{5}{11} + \frac{6}{11} \log_2 \frac{6}{11})$$

$$Ent(D) \approx - (0.455 \times -1.138 + 0.545 \times -0.874) \approx \mathbf{0.994}$$

#### 第二步：计算各属性的信息增益，选择根节点

我们需要分别计算三个属性（纹理、色泽、触感）的信息增益。

1. 属性“纹理” ($A_1$)

- 清晰 ($D^1$): {1, 2, 3, 4, 5} 共 5 个。好瓜: {1, 2, 5} (3个), 坏瓜: {3, 4} (2个)。

  $$Ent(D^1) = -(\frac{3}{5}\log_2\frac{3}{5} + \frac{2}{5}\log_2\frac{2}{5}) \approx 0.971$$

- 模糊 ($D^2$): {6, 7, 8, 9, 10, 11} 共 6 个。好瓜: {6, 7} (2个), 坏瓜: {8, 9, 10, 11} (4个)。

  $$Ent(D^2) = -(\frac{2}{6}\log_2\frac{2}{6} + \frac{4}{6}\log_2\frac{4}{6}) \approx 0.918$$

**信息增益**：

$$Gain(D, \text{纹理}) = Ent(D) - \sum \frac{|D^v|}{|D|} Ent(D^v)$$

$$Gain(D, \text{纹理}) = 0.994 - (\frac{5}{11} \times 0.971 + \frac{6}{11} \times 0.918) = 0.994 - 0.942 = \mathbf{0.052}$$

2. 属性“色泽” ($A_2$)

- 青绿 ($D^1$): {1, 2, 5, 9} 共 4 个。好瓜: {1, 2, 5} (3个), 坏瓜: {9} (1个)。

  $$Ent(D^1) = -(\frac{3}{4}\log_2\frac{3}{4} + \frac{1}{4}\log_2\frac{1}{4}) \approx 0.811$$

- 浅白 ($D^2$): {3, 6, 7, 10} 共 4 个。好瓜: {6, 7} (2个), 坏瓜: {3, 10} (2个)。

  $$Ent(D^2) = -(\frac{2}{4}\log_2\frac{2}{4} + \frac{2}{4}\log_2\frac{2}{4}) = 1.000$$

- 乌黑 ($D^3$): {4, 8, 11} 共 3 个。好瓜: 0个, 坏瓜: 3个。

  $$Ent(D^3) = 0$$ (纯度最高)

**信息增益**：

$$Gain(D, \text{色泽}) = 0.994 - (\frac{4}{11} \times 0.811 + \frac{4}{11} \times 1.000 + \frac{3}{11} \times 0)$$

$$Gain(D, \text{色泽}) = 0.994 - (0.295 + 0.364 + 0) = 0.994 - 0.659 = \mathbf{0.335}$$

3. 属性“触感” ($A_3$)

- 硬滑 ($D^1$): {1, 6, 7, 11} 共 4 个。好瓜: {1, 6, 7} (3个), 坏瓜: {11} (1个)。

  $$Ent(D^1) \approx 0.811$$

- 软粘 ($D^2$): {2, 3, 4, 5, 8, 9, 10} 共 7 个。好瓜: {2, 5} (2个), 坏瓜: {3, 4, 8, 9, 10} (5个)。

  $$Ent(D^2) = -(\frac{2}{7}\log_2\frac{2}{7} + \frac{5}{7}\log_2\frac{5}{7}) \approx 0.863$$

**信息增益**：

$$Gain(D, \text{触感}) = 0.994 - (\frac{4}{11} \times 0.811 + \frac{7}{11} \times 0.863) = 0.994 - 0.844 = \mathbf{0.150}$$

**结论**：比较三个属性的增益，$0.335 > 0.150 > 0.052$，**选择“色泽”作为根节点**。

#### 第三步：生成分支，递归构建子树

根据“色泽”将数据集划分为三个子集：

1. **色泽 = 乌黑**：样本 {4, 8, 11}。全部为坏瓜，纯度为 100%。
   - **生成叶节点：坏瓜**。

#### 第四步：最终决策树结构

色泽？
├── 乌黑 ──> 坏瓜
├── 青绿 ──> 纹理？
│           ├── 清晰 ──> 好瓜
│           └── 模糊 ──> 坏瓜
└── 浅白 ──> 触感？
            ├── 硬滑 ──> 好瓜
            └── 软粘 ──> 坏瓜



### 7.有一个5分类任务，输入一个样例后，得到输出logits＝［0.01，—0.01，—0.05，0.02，0.1］，请计算其SoftMax分类概率；如其one—hot标签label＝［0，0，0，0，1］，请计算其交叉熵损失。

<img src="Z:\课程作业\深度学习\资料\img\image-20251130223449066.png" alt="image-20251130223449066" style="zoom:33%;" />





## 四．设计题（每题20分，共40分）

### 1．请给出对大量图像进行目标检测的设计方案，要求有自己的新思路和新观点。

**设计观点与思路：** 针对大规模图像处理对实时性和精度的双重挑战，本方案选取 **One-Stage 检测架构（改进版 YOLO）** 作为基座模型以保障推理速度。创新点在于**嵌入自定义的混合注意力模块**。通过串联通道注意力和空间注意力，使网络在特征提取阶段自动抑制海量数据中的背景噪声，聚焦关键目标区域，从而在极低计算成本下提升检测精度。

```python
import torch
import torch.nn as nn
class MixedAttention(nn.Module):
    # 自定义混合注意力模块：结合通道与空间注意力
    def __init__(self, in_channels):
        super(MixedAttention, self).__init__()
        # 通道注意力：压缩通道信息，学习各通道重要性权重
        self.channel_att = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(in_channels, in_channels // 16, 1),
            nn.ReLU(),
            nn.Conv2d(in_channels // 16, in_channels, 1),
            nn.Sigmoid()
        )
        # 空间注意力：利用大卷积核提取空间位置权重
        self.spatial_att = nn.Sequential(
            nn.Conv2d(2, 1, kernel_size=7, padding=3),
            nn.Sigmoid()
        )
    def forward(self, x):
        # 1. 通道维度加权
        channel_weight = self.channel_att(x)
        x = x * channel_weight
        # 2. 空间维度加权
        # 提取最大值与均值特征进行拼接
        max_pool, _ = torch.max(x, dim=1, keepdim=True)
        avg_pool = torch.mean(x, dim=1, keepdim=True)
        spatial_map = torch.cat([max_pool, avg_pool], dim=1)
        
        spatial_weight = self.spatial_att(spatial_map)
        return x * spatial_weight
```

**实施方案：** 将上述 `MixedAttention` 模块插入到 CSPDarknet 主干网络的最后三个特征输出层（C3, C4, C5）之后，再输入 Neck 网络进行特征融合。





### 2．请给出机器阅读理解模型的设计方案，要求有自己的新思路和新观点。

#### 方案一：交互式注意力增强模型

传统的 BERT 模型仅通过 `[SEP]` 拼接问题和文章，依赖 Self-Attention 进行隐式交互，缺乏针对性的显式推理。本方案在 BERT 输出层之上引入 **双向注意力流 (Co-Attention)** 机制。通过计算“文章对问题”和“问题对文章”的相互注意力矩阵，显式地将问题的语义特征融合到文章的每个 Token 表示中，增强模型寻找答案边界的准确性。

```python
import torch
import torch.nn as nn

class CoAttentionModule(nn.Module):
    # 核心改进：显式建模文章(Context)与问题(Query)的交互
    def __init__(self, hidden_size):
        super(CoAttentionModule, self).__init__()
        # 用于计算相似度矩阵的线性映射
        self.W = nn.Linear(hidden_size, hidden_size, bias=False)

    def forward(self, context, query):
        # context: [batch, c_len, dim], query: [batch, q_len, dim]        
        # 1. 计算相似度矩阵 S = C * W * Q^T
        # 结果维度: [batch, c_len, q_len]
        proj_context = self.W(context) 
        similarity = torch.matmul(proj_context, query.transpose(1, 2))        
        # 2. Context-to-Query Attention (C2Q)
        # 这里的含义是：对于文章中的每个词，找到问题中最相关的词
        c2q_att = torch.softmax(similarity, dim=-1) # 对query维度归一化
        # 加权求和得到融合了问题信息的文章表示
        c2q_out = torch.matmul(c2q_att, query)
        # 3. 融合原始文章信息与注意力信息
        # 简单的融合方式：拼接 + 乘法 (Element-wise interaction)
        combined = torch.cat([context, c2q_out, context * c2q_out], dim=-1)        
        return combined
```

#### 方案二：门控特征过滤模型 (Gated-Fusion Reader)

在机器阅读理解中，文章通常很长，其中包含大量与问题无关的噪音句子。本方案设计一个 **相关性门控单元 (Relevance Gate)**。利用问题的特征向量生成一个 0~1 之间的“门控系数”，对文章的特征进行动态过滤。如果某段落与问题语义不符，门控将其特征抑制（置零），从而减少错误答案的干扰。

```python
class RelevanceGate(nn.Module):
    # 核心改进：利用问题特征对文章特征进行筛选
    def __init__(self, hidden_size):
        super(RelevanceGate, self).__init__()
        self.gate_fc = nn.Linear(hidden_size * 2, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, context, query_pool):
        # context: [batch, seq_len, dim]
        # query_pool: [batch, dim] (通常取问题的 [CLS] 或平均池化)        
        # 1. 将问题向量扩展到与文章相同的序列长度
        query_expanded = query_pool.unsqueeze(1).expand(-1, context.size(1), -1)     
        # 2. 拼接文章与问题特征，输入全连接层计算相关性
        cat_feature = torch.cat([context, query_expanded], dim=-1)
        # 3. 生成门控系数 (0~1)
        gate_score = self.sigmoid(self.gate_fc(cat_feature))
        # 4. 对文章特征进行"软"过滤
        # 不相关的部分会被 gate_score 缩小
        filtered_context = context * gate_score
        return filtered_context
```

#### 方案三：局部卷积增强模型 

Transformer 的 Self-Attention 擅长捕捉全局依赖，但在提取局部短语结构（如名词短语、固定搭配）时不如卷积神经网络（CNN）高效。本方案在 Transformer 输出后并在预测层之前，并行加入 **深度可分离卷积 (Depthwise Separable Conv)**。通过结合全局注意力特征与局部卷积特征，强化模型对答案边界（Start/End Position）的精确感知。

```python
class LocalConvEnhancer(nn.Module):
    # 核心改进：引入卷积捕捉局部短语特征
    def __init__(self, hidden_size, kernel_size=5):
        super(LocalConvEnhancer, self).__init__()
        # 深度卷积：分组数等于通道数，独立处理每个通道，参数少效率高
        self.depth_conv = nn.Conv1d(
            hidden_size, hidden_size, 
            kernel_size=kernel_size, 
            padding=kernel_size//2, 
            groups=hidden_size
        )
        self.point_conv = nn.Conv1d(hidden_size, hidden_size, kernel_size=1)
        self.relu = nn.ReLU()

    def forward(self, x):
        # x: [batch, seq_len, dim] -> 转换维度适配 Conv1d [batch, dim, seq_len]
        x_trans = x.transpose(1, 2)
        
        # 1. 卷积操作提取局部特征
        conv_out = self.depth_conv(x_trans)
        conv_out = self.relu(self.point_conv(conv_out))
        
        # 2. 残差连接：将局部特征加回原始全局特征
        # 维度还原: [batch, dim, seq_len] -> [batch, seq_len, dim]
        out = x + conv_out.transpose(1, 2)
        
        return out
```

### 3．请给出姿态估计模型的设计方案，要求有自己的新思路和新观点。

#### 方案一：基于 Transformer 的全局上下文增强模型

**设计观点与思路：** 传统 CNN 在姿态估计中受限于局部感受野，难以处理肢体遮挡或复杂动作（如手脚交叉）。本方案采用 **Hybrid 架构**：以 ResNet 为主干提取特征，但在输出热图（Heatmap）之前，插入一个 **Pose Transformer 模块**。利用 Self-Attention 机制捕捉所有关节点之间的全局依赖关系（如左手位置对右脚位置的隐含约束），从而显著提升在遮挡场景下的鲁棒性。

```
import torch
import torch.nn as nn

class PoseTransformerBlock(nn.Module):
    # 核心改进：在特征图上应用 Self-Attention 捕捉关节间的全局关系
    def __init__(self, channels, num_heads=4):
        super(PoseTransformerBlock, self).__init__()
        self.norm1 = nn.LayerNorm(channels)
        self.attn = nn.MultiheadAttention(embed_dim=channels, num_heads=num_heads)
        self.norm2 = nn.LayerNorm(channels)
        # 简单的前馈网络
        self.ffn = nn.Sequential(
            nn.Linear(channels, channels * 4),
            nn.ReLU(),
            nn.Linear(channels * 4, channels)
        )

    def forward(self, x):
        # x shape: [Batch, Channels, Height, Width]
        b, c, h, w = x.shape
        # 1. 展平特征图以适配 Transformer: [Seq_len (H*W), Batch, Channels]
        flat_x = x.flatten(2).permute(2, 0, 1)
        
        # 2. Self-Attention (引入全局上下文)
        # 残差连接 + LayerNorm
        res = flat_x
        flat_x = self.norm1(flat_x)
        attn_out, _ = self.attn(flat_x, flat_x, flat_x)
        flat_x = res + attn_out
        
        # 3. Feed Forward Network
        res = flat_x
        flat_x = self.norm2(flat_x)
        flat_x = res + self.ffn(flat_x)
        
        # 4. 恢复空间维度: [Batch, Channels, Height, Width]
        return flat_x.permute(1, 2, 0).view(b, c, h, w)
```

**实施位置：** 插入在 Backbone 最后一层输出之后，Head（1x1 卷积生成热图）之前。



#### 方案二：轻量级高分辨率注意力网络

**设计观点与思路：** 针对姿态估计对空间精度要求极高的特点，模仿 HRNet（高分辨率网络）保持高分辨率特征图的思路，但为了降低计算量，设计 **多尺度注意力融合模块 (MSAF)**。在低分辨率特征向高分辨率特征上采样融合时，不直接相加，而是通过注意力机制学习“融合权重”，让网络自动判断哪些尺度的特征对当前关键点定位更重要。

```python
class MultiScaleAttentionFusion(nn.Module):
    # 核心改进：在特征融合时引入注意力权重
    def __init__(self, high_dim, low_dim):
        super(MultiScaleAttentionFusion, self).__init__()
        # 将低分辨率特征调整通道数
        self.conv_low = nn.Conv2d(low_dim, high_dim, 1)
        # 空间注意力门控：决定融合时的空间权重
        self.spatial_gate = nn.Sequential(
            nn.Conv2d(2, 1, kernel_size=7, padding=3),
            nn.Sigmoid()
        )

    def forward(self, x_high, x_low):
        # x_high: 高分辨率特征
        # x_low: 低分辨率特征
        
        # 1. 上采样低分辨率特征并对齐通道
        x_low_up = F.interpolate(self.conv_low(x_low), size=x_high.shape[2:], mode='bilinear')
        
        # 2. 计算融合后的空间注意力图
        # 在通道维度拼接 max 和 avg 特征
        max_pool, _ = torch.max(x_low_up, dim=1, keepdim=True)
        avg_pool = torch.mean(x_low_up, dim=1, keepdim=True)
        spatial_weight = self.spatial_gate(torch.cat([max_pool, avg_pool], dim=1))
        
        # 3. 加权融合：高分辨率特征 + 加权后的低分辨率特征
        return x_high + x_low_up * spatial_weight
```

#### 方案三：微分坐标回归模型

**设计观点与思路：** 传统姿态估计生成 Heatmap 后需要 argmax 获取坐标，存在量化误差（精度受限于热图分辨率）。本方案摒弃 Heatmap 监督，设计 **Soft-Argmax 回归头**。通过计算特征图的“空间期望”直接得到连续的 $(x, y)$ 坐标。这使得模型完全可微，能够实现**亚像素级（Sub-pixel）**的定位精度，且大幅降低了显存占用。

```py
class SoftArgmax2D(nn.Module):
    # 核心改进：从热图直接回归连续坐标，实现端到端微分
    def __init__(self, height, width):
        super(SoftArgmax2D, self).__init__()
        self.h = height
        self.w = width
        # 生成固定的坐标网格缓冲区 [1, 1, H, W]
        pos_x, pos_y = torch.meshgrid(torch.linspace(-1, 1, height), torch.linspace(-1, 1, width))
        self.register_buffer('pos_x', pos_x.unsqueeze(0).unsqueeze(0))
        self.register_buffer('pos_y', pos_y.unsqueeze(0).unsqueeze(0))

    def forward(self, heatmaps):
        # heatmaps: [Batch, Num_Joints, H, W]
        
        # 1. 对热图进行 Softmax 归一化，将其转化为概率分布
        # 将 H*W 展平计算 softmax
        b, c, h, w = heatmaps.shape
        probs = F.softmax(heatmaps.view(b, c, -1), dim=2).view(b, c, h, w)
        
        # 2. 计算空间期望 (Expectation)
        # 坐标 = sum(概率 * 网格位置)
        coord_x = torch.sum(probs * self.pos_x, dim=[2, 3])
        coord_y = torch.sum(probs * self.pos_y, dim=[2, 3])
        
        # 返回形状: [Batch, Num_Joints, 2] -> (x, y)
        return torch.stack([coord_x, coord_y], dim=-1)
```



### 4．请给出图像描述模型的设计方案，要求有自己的新思路和新观点

#### 方案一：基于纯 Transformer 的端到端生成模型

**设计观点与思路：** 传统模型使用 CNN 提取网格特征，存在感受野局限；使用 RNN 生成文本则存在长距离遗忘问题。本方案采用 **完全基于 Transformer 的架构（ViT + Transformer Decoder）**。

- **新思路**：利用 Vision Transformer (ViT) 将图像分割为 Patch 序列，直接通过 Self-Attention 捕捉图像的全局依赖。
- **解码器**：使用标准 Transformer Decoder，通过 Cross-Attention 机制，让每个生成的单词都能关注到图像中任意位置的 Patch，实现全局信息的无损传递。

```python
class ImageCaptionTransformer(nn.Module):
    # 核心设计：利用 Transformer Decoder 进行图像到文本的转化
    def __init__(self, vocab_size, d_model, nhead, num_layers):
        super().__init__()
        # 1. 图像编码 (假设输入已经是 ViT 提取的 Patch 序列特征 [Batch, Seq_Img, Dim])
        self.visual_proj = nn.Linear(d_model, d_model)
        
        # 2. 文本嵌入
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model) # 假设已定义位置编码
        
        # 3. Transformer 解码器 (核心)
        # 包含 Self-Attention (文本内部关系) 和 Cross-Attention (文本查询图像)
        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead)
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)
        
        # 4. 输出层
        self.fc_out = nn.Linear(d_model, vocab_size)

    def forward(self, img_features, captions, tgt_mask):
        # img_features: [Seq_Img, Batch, Dim]
        # captions: [Seq_Text, Batch]
        
        # 文本嵌入 + 位置编码
        tgt = self.embedding(captions) * math.sqrt(self.d_model)
        tgt = self.pos_encoder(tgt)
        
        # 解码过程：
        # memory=img_features (图像作为 Key/Value)
        # tgt=captions (文本作为 Query)
        output = self.transformer_decoder(tgt=tgt, memory=img_features, tgt_mask=tgt_mask)
        
        return self.fc_out(output)
```

#### 方案二：自底向上注意力模型 (Bottom-Up Attention)

设计观点与思路：

传统的“空间注意力”机制是在固定的 $14 \times 14$ 特征图上进行加权，往往导致注意力分散在背景上。本方案采用 自底向上 (Bottom-Up) 注意力。

- **新思路**：利用目标检测器（如 Faster R-CNN）提取图像中 $k$ 个显著物体（Objects）的特征向量。
- **注意力计算**：注意力机制不再在网格上计算，而是在这些“物体对象”上计算。解码器（LSTM/Transformer）在生成单词时，动态决定关注图像中的“哪个人”或“哪个球”，从而生成更具语义细节的描述。

```python
class BottomUpAttention(nn.Module):
    # 核心设计：在物体特征(Object Features)上计算注意力
    def __init__(self, hidden_size, feature_dim):
        super(BottomUpAttention, self).__init__()
        # 将 LSTM 的隐藏状态 h 映射到注意力空间
        self.query_layer = nn.Linear(hidden_size, hidden_size)
        # 将物体特征 v 映射到注意力空间
        self.key_layer = nn.Linear(feature_dim, hidden_size)
        # 计算注意力分数的层
        self.energy_layer = nn.Linear(hidden_size, 1)

    def forward(self, hidden_state, object_features):
        # hidden_state: [Batch, Hidden] (当前时刻解码器的状态)
        # object_features: [Batch, K_Objects, Feat_Dim] (R-CNN 提取的物体特征)
        
        # 1. 扩展 query 维度以匹配物体数量
        # query: [Batch, 1, Hidden]
        query = self.query_layer(hidden_state).unsqueeze(1)
        
        # 2. 计算能量分数 (Energy)
        # keys: [Batch, K, Hidden]
        keys = self.key_layer(object_features)
        
        # 这里的 tanh 是经典 additive attention
        energy = torch.tanh(query + keys) 
        scores = self.energy_layer(energy).squeeze(2) # [Batch, K]
        
        # 3. 获取注意力权重 (Alpha)
        alpha = F.softmax(scores, dim=1)
        
        # 4. 加权求和得到上下文向量 (Context Vector)
        # context: [Batch, Feat_Dim]
        context = torch.bmm(alpha.unsqueeze(1), object_features).squeeze(1)
        
        return context, alpha
```

#### 方案三：语义概念引导模型

**设计观点与思路：** 仅靠图像特征有时难以准确识别抽象概念或细微属性。本方案设计 **语义引导融合模块**。

- **新思路**：第一步先利用多标签分类网络，从图像中预测出一组高频语义标签（如“草地”、“奔跑”、“狗”）。第二步将这些**语义标签的 Embedding** 与图像的视觉特征进行融合，作为解码器的输入。
- **作用**：语义标签提供了显式的“先验知识”，指导模型在生成句子时准确使用这些关键词，减少幻觉（Hallucination）。

```python
class SemanticGuidedFusion(nn.Module):
    # 核心设计：融合视觉特征与语义标签特征
    def __init__(self, visual_dim, semantic_dim, embed_dim):
        super(SemanticGuidedFusion, self).__init__()
        # 语义标签的 Embedding 层
        self.tag_embedding = nn.Embedding(num_tags, semantic_dim)
        
        # 融合门控 (Gating Mechanism)
        self.visual_gate = nn.Linear(visual_dim, embed_dim)
        self.semantic_gate = nn.Linear(semantic_dim, embed_dim)
        self.sigmoid = nn.Sigmoid()
        
        self.output_fc = nn.Linear(visual_dim + semantic_dim, embed_dim)

    def forward(self, visual_feat, predicted_tags):
        # visual_feat: [Batch, V_Dim] (CNN 提取的图像特征)
        # predicted_tags: [Batch, Top_K] (预测出的概率最高的 K 个标签索引)
        
        # 1. 获取语义向量 (将 K 个标签的 Embedding 平均或拼接)
        tag_embeds = self.tag_embedding(predicted_tags) # [Batch, K, S_Dim]
        semantic_feat = torch.mean(tag_embeds, dim=1)   # [Batch, S_Dim]
        
        # 2. 简单的拼接融合
        combined = torch.cat([visual_feat, semantic_feat], dim=1)
        
        # 3. (可选) 进阶思路：利用门控机制动态决定信赖视觉还是信赖语义
        # gate = sigmoid(W_v * v + W_s * s)
        # output = gate * visual + (1-gate) * semantic
        
        return self.output_fc(combined)
```

##### 

### 5．请给出视频超分辨率模型的设计方案，要求有自己的新思路和新观点。

#### 方案一：轨迹感知变形 Transformer 

**设计观点与思路：** 视频超分的难点在于相邻帧之间存在大运动，直接堆叠 Transformer 计算量过大且对齐困难。本方案提出 **轨迹感知注意力机制 (Trajectory Attention)**。不再对全图计算 Attention，而是利用 **可变形偏移 (Deformable Offset)** 预测物体在时序上的运动轨迹，仅在轨迹路径上的关键点进行稀疏 Attention 聚合。这既实现了隐式的时间对齐，又捕捉了长距离的时序纹理。

```python
import torch
import torch.nn as nn
class TrajectoryAttention(nn.Module):
    # 核心创新：结合可变形卷积(DCN)思想的稀疏注意力
    def __init__(self, dim, num_heads):
        super().__init__()
        self.num_heads = num_heads
        # 预测时序上的偏移量 (Offset)
        # 输入当前帧特征，预测参考帧上的采样位置偏移
        self.offset_net = nn.Conv2d(dim, 2 * num_heads, kernel_size=3, padding=1)
        
        # 定义 Query, Key, Value 映射
        self.to_q = nn.Linear(dim, dim)
        self.to_k = nn.Linear(dim, dim)
        self.to_v = nn.Linear(dim, dim)
    def forward(self, curr_feat, ref_feats):
        # curr_feat: 当前帧 [B, C, H, W]
        # ref_feats: 参考帧序列 [B, T, C, H, W]

        b, c, h, w = curr_feat.shape
        # 1. 计算偏移量，找到物体在参考帧中的"轨迹位置"
        offsets = self.offset_net(curr_feat) # [B, 2*Heads, H, W]
        
        # 2. 基于偏移量，在参考帧上进行双线性采样 (Grid Sample)
        # 这一步实现了"隐式对齐"，将参考帧特征对齐到当前帧坐标
        # aligned_refs: [B, T, C, H, W] (采样后的特征)
        aligned_refs = bilinear_sample(ref_feats, offsets) 
        
        # 3. 在对齐后的轨迹点上计算 Attention
        q = self.to_q(curr_feat)
        k = self.to_k(aligned_refs)
        v = self.to_v(aligned_refs)
        
        # 标准 Attention: Q * K^T
        attn = (q @ k.transpose(-2, -1)) * (self.dim ** -0.5)
        attn = attn.softmax(dim=-1)
        
        # 聚合时序信息
        return (attn @ v) + curr_feat
```

#### 方案二：双向流引导循环网络

设计观点与思路：

Transformer 虽然强但显存占用高。本方案回归 RNN 架构，采用双向传播（Bi-directional Propagation）。

- **新思路**：在隐藏状态（Hidden State）传播时，引入 **光流引导的变形对齐 (Flow-Guided Alignment)**。不同于传统 RNN 直接拼接上一帧特征，本方案先利用光流将 $H_{t-1}$ Warp 到当前时刻 $t$，再进行卷积更新。这解决了 RNN 处理大运动时的“重影”和特征错位问题。

```
class FlowGuidedRNNBlock(nn.Module):
    # 核心设计：带有光流对齐的循环单元
    def __init__(self, channels):
        super().__init__()
        # 光流估计网络 (如 SPyNet 的轻量版)
        self.flow_net = SPyNetPretrained() 
        # 融合卷积：融合对齐后的历史信息和当前输入
        self.fusion_conv = nn.Conv2d(channels * 3, channels, 3, padding=1)

    def forward(self, current_feat, hidden_state, flow=None):
        # current_feat: 当前帧特征
        # hidden_state: 上一时刻的记忆特征
        
        # 1. 光流估计 (如果未预先计算)
        # 计算从 t-1 到 t 的运动
        if flow is None:
            flow = self.flow_net(current_feat, hidden_state)
            
        # 2. 特征变形 (Warping)
        # 关键点：将上一时刻的记忆"搬运"到当前时刻的位置
        aligned_hidden = flow_warp(hidden_state, flow)
        
        # 3. 融合更新
        # 拼接：当前输入 + 对齐后的记忆 + 原始记忆(残差)
        combined = torch.cat([current_feat, aligned_hidden, hidden_state], dim=1)
        new_hidden = self.fusion_conv(combined)
        
        return new_hidden # 输出作为下一时刻的输入，并用于重建
```

#### 方案三：时空分组 3D 卷积网络

**设计观点与思路：** 3D 卷积（C3D）能同时提取时空特征，但参数量巨大。本方案提出 **分组时空分离 (Grouped ST-Separation)** 策略。

- **新思路**：将特征通道分组，一组只负责“空间纹理提取”（2D Conv），另一组只负责“时间差分提取”（1D Temporal Conv）。
- **创新点**：引入 **时间差分注意力 (Temporal Difference Attention)**，显式增强运动剧烈的区域（通常是模糊重灾区）的权重，使网络专注于恢复运动模糊。

```python
class GroupedSTBlock(nn.Module):
    # 核心设计：分组处理空间与时间，降低 3D 卷积成本
    def __init__(self, dim):
        super().__init__()
        # 空间分支 (处理纹理)
        self.spatial_conv = nn.Conv2d(dim // 2, dim // 2, 3, padding=1)
        # 时间分支 (处理运动，模拟 1x1x3 的 3D 卷积)
        self.temporal_conv = nn.Conv3d(dim // 2, dim // 2, kernel_size=(3, 1, 1), padding=(1, 0, 0))
        
        # 融合层
        self.fusion = nn.Conv2d(dim, dim, 1)

    def forward(self, x):
        # x: [Batch, Time, Channel, H, W]
        b, t, c, h, w = x.shape
        
        # 1. 通道分组
        x_s, x_t = torch.split(x, c // 2, dim=2)
        
        # 2. 空间路径：将 Time 维度折叠进 Batch 处理
        # [B*T, C/2, H, W]
        out_s = self.spatial_conv(x_s.reshape(-1, c//2, h, w))
        out_s = out_s.reshape(b, t, c//2, h, w)
        
        # 3. 时间路径：使用 3D 卷积提取帧间关系
        # [B, C/2, T, H, W] (需转置适配 Conv3d)
        out_t = self.temporal_conv(x_t.permute(0, 2, 1, 3, 4)).permute(0, 2, 1, 3, 4)
        # 4. 拼接融合
        out = torch.cat([out_s, out_t], dim=2)
        # 再次折叠维度进行 1x1 融合
        return self.fusion(out.reshape(-1, c, h, w)).reshape(b, t, c, h, w) + x
```



### 6．请给出图像分割的设计方案，写出代码并注释，要求有自己的新思路和新观点。

#### 方案一：边缘增强型混合 Transformer

**设计观点与思路：** 纯 Transformer 架构（如 SegFormer）虽然能捕捉全局上下文，但在上采样过程中容易丢失高频细节，导致分割边缘锯齿化。 **个人改进（新思路）**：设计一个 **边缘感知融合模块 (Edge-Aware Fusion Module)**。在解码器阶段，不直接将 Encoder 的多尺度特征拼接，而是先利用 CNN 分支提取边缘信息（如通过 Sobel 算子或差分特征），以此生成“边缘注意力权重”，对 Transformer 的特征进行加权修正，强制模型关注物体轮廓。

```
import torch
import torch.nn as nn
import torch.nn.functional as F

class EdgeAwareFusion(nn.Module):
    # 核心创新：在 Transformer 特征融合时引入边缘注意力
    def __init__(self, dim):
        super(EdgeAwareFusion, self).__init__()
        # 简单的卷积层用于特征对齐
        self.conv_trans = nn.Conv2d(dim, dim, 1)
        self.conv_cnn = nn.Conv2d(dim, dim, 1)
        # 边缘注意力生成器
        self.edge_gate = nn.Sequential(
            nn.Conv2d(dim, 1, kernel_size=3, padding=1),
            nn.Sigmoid()
        )
    def forward(self, trans_feat, cnn_feat_high_res):
        # trans_feat: 来自 Transformer 的深层语义特征 (低分辨率)
        # cnn_feat_high_res: 来自 CNN 的浅层细节特征 (高分辨率)
        
        # 1. 上采样 Transformer 特征以匹配分辨率
        trans_up = F.interpolate(self.conv_trans(trans_feat), 
                                 size=cnn_feat_high_res.shape[2:], 
                                 mode='bilinear', align_corners=False)
        # 2. 核心改进：计算边缘图
        # 利用浅层 CNN 特征的丰富细节，提取高频边缘信息
        # 也可以显式使用 Sobel 算子，这里用可学习卷积模拟
        edge_map = self.edge_gate(cnn_feat_high_res)
        
        # 3. 边缘增强融合
        # 在边缘区域，赋予 CNN 特征更高的权重；在平滑区域，依赖 Transformer 上下文
        # fused = Trans * (1 - edge) + CNN * edge
        fused = trans_up * (1 - edge_map) + self.conv_cnn(cnn_feat_high_res) * edge_map
        
        return fused
```

#### 方案二：动态原型语义分割网络

**设计观点与思路：** 传统的 FCN/U-Net 最后使用固定的 $1 \times 1$ 卷积作为分类器。然而，同一类物体（如“路面”）在不同光照（白天/黑夜）下的特征分布差异巨大，固定权重难以适应。 **个人改进（新思路）**：提出 **动态原型学习 (Dynamic Prototype Learning)**。通过全局池化提取当前图像的“环境上下文”，动态生成最后一层分类器的权重（即类别原型）。这使得分割头能根据当前图片的整体风格自适应调整，提高泛化能力。

```
class DynamicHead(nn.Module):
    # 核心创新：根据图像内容动态生成分类器权重
    def __init__(self, in_channels, num_classes, reduction=16):
        super(DynamicHead, self).__init__()
        self.num_classes = num_classes
        self.in_channels = in_channels
        
        # 权重生成网络 (HyperNetwork 思想)
        # 输入全局特征，输出卷积核权重
        self.weight_gen = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(in_channels, in_channels // reduction, 1),
            nn.ReLU(),
            nn.Conv2d(in_channels // reduction, in_channels * num_classes, 1)
        )

    def forward(self, x):
        # x: [Batch, In_Channels, H, W] (解码器的输出特征)
        batch_size, _, h, w = x.shape
        
        # 1. 生成动态权重
        # weights shape: [Batch, In_C * Num_Class, 1, 1]
        weights = self.weight_gen(x)
        
        # 2. 重塑权重以适配 Group Convolution
        # 目标: 对每个样本分别应用不同的卷积核
        # Reshape to: [Batch * Num_Class, In_Channels, 1, 1]
        weights = weights.view(batch_size * self.num_classes, self.in_channels, 1, 1)
        
        # 3. 调整输入数据格式
        # Input reshape: [1, Batch * In_Channels, H, W]
        x_reshaped = x.view(1, batch_size * self.in_channels, h, w)
        
        # 4. 应用动态卷积 (利用分组卷积实现逐样本处理)
        # groups = batch_size，实现每个样本只用自己对应的权重进行卷积
        logits = F.conv2d(x_reshaped, weights, groups=batch_size)
        
        # Output reshape: [Batch, Num_Class, H, W]
        return logits.view(batch_size, self.num_classes, h, w)
```

方案三：多尺度频率门控 U-Net 

**设计观点与思路：** U-Net 的跳跃连接（Skip Connection）直接将编码器的特征拼接到解码器，往往引入了过多的背景噪声。 **个人改进（新思路）**：在跳跃连接中引入 **频率门控机制 (Frequency Gate)**。利用 FFT（快速傅里叶变换）将特征转到频域，分离出高频（纹理/边界）和低频（平滑背景）。通过可学习的滤波器抑制低频背景噪声，仅允许高频的边界细节通过跳跃连接传递给解码器，从而提升分割精度。

```
class FrequencyGatedSkip(nn.Module):
    # 核心创新：在频域对跳跃连接特征进行滤波
    def __init__(self, channels):
        super(FrequencyGatedSkip, self).__init__()
        # 可学习的频域滤波器 (Mask)
        # 简单起见，这里假设特征图大小固定，或者是自适应调整的参数
        self.frequency_mask = nn.Parameter(torch.ones(1, channels, 1, 1))

    def forward(self, x):
        # x: [Batch, C, H, W] (Encoder 层的特征)
        
        # 1. 快速傅里叶变换 (转到频域)
        # rfft2d 输出实部和虚部
        x_fft = torch.fft.rfft2(x, norm='ortho')
        
        # 2. 应用可学习的频域滤波
        # 这里模拟高通滤波：模型会自动学习抑制低频(中心区域)幅度
        # 创新点：让网络自己决定保留哪些频率成分
        x_fft_filtered = x_fft * self.frequency_mask
        
        # 3. 逆傅里叶变换 (转回空域)
        x_filtered = torch.fft.irfft2(x_fft_filtered, s=x.shape[-2:], norm='ortho')
        
        # 残差连接：保留原始信息 + 滤波后的高频细节
        return x + x_filtered
```

### 7．请给出神经机器翻译的设计方案，写出代码并注释，要求有自己的新思路和新观点。

#### 方案一：动态上下文门控 Transformer

**设计观点与思路：** 标准的 Transformer 利用 Self-Attention 捕捉全局依赖，但在翻译长难句时，某个词往往只与周围的几个词（局部短语）关系最密切，过度的全局关注反而引入噪音。 **个人改进（新思路）**：在 Self-Attention 之后引入一个 **上下文门控单元 (Context Gate)**。它根据当前词的特征，自适应地学习一个权重 $\lambda$，决定是保留更多的 **全局上下文（Attention 输出）** 还是 **局部原始信息（Residual 输入）**，从而实现“该看全局看全局，该看局部看局部”。

```
import torch
import torch.nn as nn

class ContextGatedAttention(nn.Module):
    # 核心创新：在注意力输出后加入动态门控，平衡全局与局部信息
    def __init__(self, d_model):
        super(ContextGatedAttention, self).__init__()
        self.self_attn = nn.MultiheadAttention(d_model, num_heads=8)
        self.norm = nn.LayerNorm(d_model)
        
        # 门控网络：学习一个 0~1 之间的系数 lambda
        self.gate_net = nn.Sequential(
            nn.Linear(d_model * 2, d_model),
            nn.Sigmoid()
        )
        self.proj = nn.Linear(d_model, d_model)

    def forward(self, x, mask=None):
        # x: [Seq_Len, Batch, Dim]
        
        # 1. 标准的多头自注意力 (捕捉全局上下文 Global Context)
        # attn_out 代表了融合了全局信息的特征
        attn_out, _ = self.self_attn(x, x, x, attn_mask=mask)
        
        # 2. 核心改进：计算上下文门控 (Context Gate)
        # 将"原始局部信息 x" 与 "全局信息 attn_out" 拼接，判断当前词需要多少全局信息
        combined = torch.cat([x, attn_out], dim=-1)
        gate = self.gate_net(combined) # [Seq_Len, Batch, Dim]
        
        # 3. 动态融合
        # output = gate * 全局信息 + (1 - gate) * 局部信息
        # 这样模型可以自动忽略不相关的长距离依赖
        fused = gate * attn_out + (1 - gate) * x
        
        # 残差连接 + 归一化
        return self.norm(x + self.proj(fused))
```

#### 方案二：检索增强型 NMT

**设计观点与思路：** 神经翻译模型通过参数记忆知识，但在遇到生僻人名或特定领域术语时容易产生“幻觉”。 **个人改进（新思路）**：设计 **外部记忆融合层 (Memory Fusion Layer)**。假设我们有一个外部的“术语库”或“相似句对库”，在解码时，先检索与当前上下文最相似的外部知识（Key-Value），然后利用 Cross-Attention 将这些检索到的**外部知识显式地融合**到解码器中，作为一种“作弊小抄”来辅助翻译。

```
class RetrievalFusionLayer(nn.Module):
    # 核心创新：将检索到的外部知识融合进解码器
    def __init__(self, d_model):
        super(RetrievalFusionLayer, self).__init__()
        # 专门用于处理检索记忆的注意力层
        self.memory_attn = nn.MultiheadAttention(d_model, num_heads=4)
        self.layer_norm = nn.LayerNorm(d_model)
        self.gate = nn.Linear(d_model * 2, 1) # 简单的融合门

    def forward(self, decoder_state, retrieved_memory):
        # decoder_state: [1, Batch, Dim] (当前解码时刻的状态)
        # retrieved_memory: [K, Batch, Dim] (从外部数据库检索到的 K 个相似向量)
        
        # 1. 利用 Cross-Attention 读取外部记忆
        # Query = 当前解码状态, Key/Value = 检索到的外部知识
        # 这一步让模型去"查阅"外部资料
        memory_context, _ = self.memory_attn(query=decoder_state, 
                                             key=retrieved_memory, 
                                             value=retrieved_memory)
        
        # 2. 核心改进：自适应融合
        # 计算一个标量权重，决定多大程度上信赖外部知识
        fusion_prob = torch.sigmoid(self.gate(torch.cat([decoder_state, memory_context], dim=-1)))
        
        # 3. 混合特征
        # 最终状态 = 原始解码状态 + 权重 * 外部知识修正
        output = decoder_state + fusion_prob * memory_context
        
        return self.layer_norm(output)
```

#### 方案三：非自回归迭代优化模型

**设计观点与思路：** 传统的 NMT 是从左到右逐词生成，速度慢且容易产生误差累积。 **个人改进（新思路）**：采用 **Mask-Predict 迭代优化策略**。 第一轮先并行生成一个粗糙的“草稿”（Draft），虽然速度快但可能语法不通。 第二轮利用模型对“草稿”中置信度较低（Low Confidence）的词进行 **Mask（掩码）**，然后依据上下文重新预测这些被 Mask 的位置。这种“先写草稿再修改”的方式更符合人类翻译习惯。

```
class IterativeRefinementDecoder(nn.Module):
    # 核心创新：基于置信度的迭代掩码与重预测
    def __init__(self, transformer_decoder, vocab_size):
        super(IterativeRefinementDecoder, self).__init__()
        self.decoder = transformer_decoder
        self.fc_out = nn.Linear(d_model, vocab_size)

    def forward(self, src_enc, target_seq, iter_num=1):
        # src_enc: 编码器输出
        # target_seq: 初始化的目标序列 (可能是全 Mask 或上一轮的草稿)
        
        for i in range(iter_num):
            # 1. 并行解码：一次性预测所有位置
            out_features = self.decoder(target_seq, src_enc)
            logits = self.fc_out(out_features) # [Seq, Batch, Vocab]
            
            # 2. 获取预测概率与结果
            probs = torch.softmax(logits, dim=-1)
            confidences, predicted_ids = torch.max(probs, dim=-1)
            
            # 如果是最后一轮，直接返回结果
            if i == iter_num - 1:
                return predicted_ids
            
            # 3. 核心改进：Mask 策略
            # 设定阈值，找出置信度低的词
            # mask_indices = confidences < threshold
            # 将这些位置替换为 [MASK] token，保留高置信度的词作为下一轮的上下文
            target_seq = self.apply_mask(predicted_ids, confidences)
            
        return predicted_ids

    def apply_mask(self, seq, confidences):
        # 模拟掩码操作：保留高分词，Mask 低分词
        mask_token_id = 0
        threshold = 0.8
        mask = confidences < threshold
        seq[mask] = mask_token_id
        return seq
```

### 8.请给出医学图像语义分割模型的设计方案，要求有自己的新思路和新观点。

#### 方案一：注意力门控 U-Net 

**设计观点与思路：** 医学图像中病灶往往只占很小一部分，标准 U-Net 的跳跃连接（Skip Connection）直接将编码器的浅层特征拼接到解码器，会引入大量无关的背景噪声。 **创新点**：在跳跃连接处引入 **注意力门控 (Attention Gate)**。利用解码器的高层特征（Gating Signal）作为查询信号，对编码器的浅层特征进行“过滤”，抑制背景区域的响应，只保留与病灶相关的空间特征。

```
import torch
import torch.nn as nn

class AttentionGate(nn.Module):
    # 核心创新：用于过滤 Skip Connection 中的背景噪声
    def __init__(self, F_g, F_l, F_int):
        super(AttentionGate, self).__init__()
        # F_g: Gating signal (解码器特征), F_l: Local feature (编码器特征)
        
        # 两个 1x1 卷积用于通道对齐
        self.W_g = nn.Conv2d(F_g, F_int, kernel_size=1)
        self.W_x = nn.Conv2d(F_l, F_int, kernel_size=1)
        
        # 生成注意力系数 (0~1)
        self.psi = nn.Sequential(
            nn.Conv2d(F_int, 1, kernel_size=1),
            nn.Sigmoid()
        )
        self.relu = nn.ReLU()

    def forward(self, g, x):
        # g: 解码器的下采样特征 (Gating)
        # x: 编码器的跳跃连接特征 (Skip)
        
        # 1. 特征相加融合 (广播机制)
        g1 = self.W_g(g)
        x1 = self.W_x(x)
        psi = self.relu(g1 + x1)
        
        # 2. 生成权重图
        scale = self.psi(psi)
        
        # 3. 对原始跳跃特征进行加权，抑制背景
        return x * scale
```

**实施位置：** 将此模块插入到 U-Net 解码器的每一次上采样（Up-sampling）之后，在与编码器特征拼接（Concat）之前。

#### 方案二：混合 Transformer U-Net

**设计观点与思路：** 纯 CNN 架构受限于局部感受野，难以建立长距离的像素依赖关系，导致在分割大面积组织或形状复杂的病灶时效果不佳。 **创新点**：采用 **CNN-Transformer 混合架构**。保持 U-Net 的整体结构，但将最底层的 **瓶颈层 (Bottleneck)** 替换为 Transformer Block。利用 Self-Attention 机制提取全局上下文信息，再通过解码器逐步恢复分辨率，实现“全局感知 + 局部精细”的分割。

```
class BottleneckTransformer(nn.Module):
    # 核心创新：在 U-Net 底部引入 Transformer 捕捉全局上下文
    def __init__(self, dim, num_heads=8):
        super(BottleneckTransformer, self).__init__()
        # 多头自注意力
        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads)
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)
        # 前馈网络
        self.ffn = nn.Sequential(
            nn.Linear(dim, dim * 4), nn.ReLU(), nn.Linear(dim * 4, dim)
        )

    def forward(self, x):
        # x: [Batch, C, H, W] (Encoder 最后一层输出)
        b, c, h, w = x.shape
        
        # 1. 展平空间维度以适配 Transformer 输入
        # [H*W, Batch, C]
        flat_x = x.flatten(2).permute(2, 0, 1)
        
        # 2. Self-Attention 计算 (全局建模)
        attn_out, _ = self.attn(flat_x, flat_x, flat_x)
        x_res = self.norm1(flat_x + attn_out)
        
        # 3. Feed Forward
        out = self.norm2(x_res + self.ffn(x_res))
        
        # 4. 恢复空间维度
        return out.permute(1, 2, 0).view(b, c, h, w)
```

#### 方案三：边缘感知多任务 U-Net 

**设计观点与思路：** 医学图像（如 CT/MRI）中，组织与组织的对比度低，导致病灶**边界极其模糊**，常规分割容易出现“溢出”或“欠分割”。 **创新点**：引入 **边缘辅助分支 (Edge Branch)**。设计一个并行的解码支路，专门用于预测图像的边缘（Edge Map）。将提取到的边缘特征融合进主分割网络，显式地强调轮廓约束。

```
class EdgeGuidanceModule(nn.Module):
    # 核心创新：显式提取边缘特征并增强主特征
    def __init__(self, in_channels):
        super(EdgeGuidanceModule, self).__init__()
        # 边缘预测分支
        self.edge_conv = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // 2, 1),
            nn.ReLU(),
            nn.Conv2d(in_channels // 2, 1, 1), # 输出单通道边缘图
            nn.Sigmoid()
        )
        
    def forward(self, x):
        # x: 解码器某层的特征
        
        # 1. 预测边缘概率图
        edge_map = self.edge_conv(x)
        
        # 2. 特征增强
        # 利用预测出的边缘图对原始特征进行加权
        # 逻辑：在边缘处增强特征响应，非边缘处保持原样
        # (1 + edge_map) 相当于一种 Residual Attention
        enhanced_feat = x * (1 + edge_map)
        
        return enhanced_feat, edge_map
```

### 9．请给出图像修复模型的设计方案，要求有自己的新思路和新观点。

#### 方案一：基于上下文注意力 GAN 的修复模型

**设计观点与思路：** 传统卷积网络在修复大面积缺失时，只能利用周围像素进行平滑扩散，导致修复区域模糊且缺乏纹理。 **新思路**：引入 **上下文注意力模块 (Contextual Attention Module)**。将图像分为“缺失区域（前景）”和“已知区域（背景）”。利用注意力机制计算前景像素与背景像素的相似度，从背景中“借用”最相似的纹理特征来填充前景，实现长距离的特征搬运。

```
import torch
import torch.nn as nn
import torch.nn.functional as F

class ContextualAttention(nn.Module):
    # 核心创新：利用已知背景区域的特征去重构缺失区域
    def __init__(self, ksize=3, stride=1):
        super(ContextualAttention, self).__init__()
        self.ksize = ksize
        self.stride = stride
        self.softmax = nn.Softmax(dim=3)

    def forward(self, x, mask):
        # x: 特征图 [B, C, H, W]
        # mask: 缺失区域掩码 (1为缺失，0为已知)
        
        # 1. 提取 Query (缺失区域) 和 Key (已知背景区域)
        # 这里简化为将特征图 unfold 成一个个 patch
        # patches: [B, C*k*k, L]
        patches = F.unfold(x, kernel_size=self.ksize, stride=self.stride)
        
        # 2. 计算相似度 (Attention Score)
        # 缺失区域的 patch 去匹配背景区域的 patch
        # score: [B, L, L] -> 表示第 i 个位置与第 j 个位置的相似度
        # 归一化特征以计算余弦相似度
        norm_patches = F.normalize(patches, dim=1)
        score = torch.bmm(norm_patches.transpose(1, 2), norm_patches)
        
        # 3. 应用 Softmax 获取权重，并处理 Mask
        # 这一步确保我们只从"已知区域"借用纹理 (Mask过滤)
        # 简单的逻辑：将 mask 区域的 score 设为负无穷
        # attention_map = self.softmax(score) ... (省略具体掩码操作)
        attention_map = self.softmax(score)
        
        # 4. 重构特征
        # 利用 attention map 加权组合背景特征
        reconstructed_patches = torch.bmm(patches, attention_map.transpose(1, 2))
        
        # 5. Fold 回去恢复成特征图形状
        y = F.fold(reconstructed_patches, output_size=x.shape[2:], 
                   kernel_size=self.ksize, stride=self.stride)
        
        return y
```

#### 方案二：门控卷积修复网络

**设计观点与思路：** 普通卷积在滑动窗口时，会将孔洞内的无效值（通常是0或均值）与有效像素一起计算，导致特征被污染。 **新思路**：提出 **门控卷积 (Gated Convolution)**。网络在学习特征的同时，自动学习一个“软掩码（Gate）”。这个 Gate 动态决定哪些像素是有效的，哪些是无效的。不仅用于填补内容，还能随着网络层数加深，自动更新掩码的形状，实现从边缘向中心的逐步修复。

```
class GatedConv2d(nn.Module):
    # 核心创新：学习一个动态的 Gate 来过滤无效特征
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super(GatedConv2d, self).__init__()
        # 定义两个卷积：一个提取特征，一个预测 Gate
        self.conv_feature = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
        self.conv_gate = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
        self.sigmoid = nn.Sigmoid()
        self.elu = nn.ELU()

    def forward(self, x):
        # x: 输入特征图
        
        # 1. 计算内容特征
        feature = self.elu(self.conv_feature(x))
        
        # 2. 计算门控系数 (Gate)
        # 输出 0~1 的值，代表该位置特征的"有效性"或"置信度"
        gate = self.sigmoid(self.conv_gate(x))
        
        # 3. 门控操作
        # 无效区域的特征会被 Gate 抑制，有效区域保留
        return feature * gate
```

### 10．请给出长文本情感分析模型的设计方案，要求有自己的新思路和新观点。给出概率前三的回答

#### 方案一：层级注意力网络 

**设计观点与思路：** BERT 等模型受限于 512 长度限制，暴力截断会丢失信息。长文本的情感往往由少数几个“关键句”和句中的“情感词”决定。 **创新点**：采用 **分层建模 + 双重注意力机制**。

1. **词级注意力 (Word Level)**：在句子内部，关注哪些词是情感词。
2. **句级注意力 (Sentence Level)**：在文档层面，关注哪些句子决定了整体情感（如开头、结尾或转折句）。

```
import torch
import torch.nn as nn
import torch.nn.functional as F

class HierarchicalAttention(nn.Module):
    # 核心设计：通用的注意力模块，可复用于 Word 层和 Sentence 层
    def __init__(self, feature_dim):
        super(HierarchicalAttention, self).__init__()
        # 这里的 u_w 是可学习的上下文向量 (Context Vector)
        # 它的作用类似于"查询(Query)"，去寻找最含信息的特征
        self.attn_vec = nn.Linear(feature_dim, feature_dim)
        self.u_w = nn.Linear(feature_dim, 1, bias=False)

    def forward(self, x):
        # x: [Batch, Seq_Len, Dim] (可以是词向量序列，也可以是句子向量序列)
        
        # 1. 非线性变换 (MLP)
        u = torch.tanh(self.attn_vec(x))
        
        # 2. 计算重要性分数 (Attention Score)
        # 将每个时间步的特征与可学习的 u_w 进行点积
        attn_weights = self.u_w(u) # [Batch, Seq_Len, 1]
        attn_weights = F.softmax(attn_weights, dim=1)
        
        # 3. 加权求和，生成更高一级的表示
        # 词向量加权 -> 句子向量；句子向量加权 -> 文档向量
        aggregated_vector = torch.sum(x * attn_weights, dim=1)
        
        return aggregated_vector, attn_weights

# 架构简述：
# 1. WordEncoder: 词嵌入 -> Bi-GRU -> WordAttention -> 得到 Sentence Vector
# 2. SentEncoder: Sentence Vectors -> Bi-GRU -> SentenceAttention -> 得到 Document Vector
# 3. Classifier: Document Vector -> Softmax
```

#### 方案二：基于关键句抽取的两阶段模型

**设计观点与思路：** 长文本中往往充斥着大量中性的背景描述，直接输入模型会稀释情感信号。 **创新点**：设计 **轻量级筛选器 + 重量级分类器** 的两阶段架构。

1. **粗读（筛选）**：使用一个简单的 CNN 或轻量级 Transformer 快速扫描全文，为每个句子打分，选出 Top-K 个最具有情感倾向的“关键句”。
2. **精读（分类）**：将选出的关键句拼接，输入到 BERT 等强力模型中进行最终判别。这既解决了长度限制，又去除了噪音。

```
class SignificanceSelector(nn.Module):
    # 核心创新：轻量级句子评分器，用于"去粗取精"
    def __init__(self, input_dim):
        super(SignificanceSelector, self).__init__()
        # 简单的门控网络，判断句子是否包含重要情感
        self.gate_net = nn.Sequential(
            nn.Linear(input_dim, input_dim // 2),
            nn.ReLU(),
            nn.Linear(input_dim // 2, 1),
            nn.Sigmoid() # 输出 0~1 的重要性概率
        )

    def forward(self, sentence_embs):
        # sentence_embs: [Batch, Num_Sentences, Dim] (预先提取的句子特征)
        
        # 1. 计算每个句子的重要性得分
        scores = self.gate_net(sentence_embs).squeeze(-1) # [Batch, Num_Sentences]
        
        # 2. 核心逻辑：Top-K 采样 (Hard Selection)
        # 选出得分最高的 K 个句子的索引
        k = 5 
        topk_scores, topk_indices = torch.topk(scores, k, dim=1)
        
        # 3. 提取关键句特征用于后续分类
        # gather 用于根据索引提取 tensor
        batch_indices = torch.arange(sentence_embs.size(0)).unsqueeze(1).expand_as(topk_indices)
        selected_sentences = sentence_embs[batch_indices, topk_indices]
        
        # 返回筛选后的特征，送入 BERT 进行精细分类
        return selected_sentences
```

